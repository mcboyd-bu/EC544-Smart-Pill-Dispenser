FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=15 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.59957047136206753635e+00) (1, -4.43264817105599817637e+00) (2, -7.81240171343453138064e-01) (3, -4.29165390167851512615e+00) (4, -1.89622048748770810622e-01) (5, -1.87059299660814115640e+00) (6, 3.46088760375752313436e+00) (7, -4.60294049779967995306e+00) (8, -3.26732681716098427582e-01) (9, -4.34376630221028392498e+00) (10, -2.17871928976356565855e+00) (11, -2.57324558658421453927e+00) (12, 1.25107287203558925626e+00) (13, -2.11746436260716608402e+00) (14, 4.57194008262534268283e+00) (0, -2.25945623868536049628e+00) (1, 1.28697065489330952914e+00) (2, 1.01506656228301683775e+00) (3, 3.07171862812771800932e+00) (4, 3.83679884856714600860e-02) (5, 1.36642827354812979035e-01) (6, -1.34889960497918748317e+00) (7, 8.62567816712037194193e-01) (8, 7.46572351094825403806e+00) (9, 2.99734970422520641264e+00) (10, 1.70664323203277046304e+00) (11, 2.68394911990699513993e+00) (12, 1.10991280591423907564e-01) (13, 1.39488268470823251199e+00) (14, -1.35993720799985551295e+00) (0, -9.78331006709684869449e-01) (1, 3.25478870263018915665e+00) (2, 1.98282433689899978413e-01) (3, 4.67667988356439690012e-01) (4, -3.79467544146591451160e-01) (5, -1.46743207756198590364e-01) (6, -5.21242968368272885371e-01) (7, 1.63877336683500929304e+00) (8, -1.60982738362125732401e+00) (9, 5.90456199356895639063e-01) (10, -9.71447169195634452343e-02) (11, 1.76547717980671453608e-01) (12, -1.10160438705464699960e-01) (13, 1.23465962947972823782e-01) (14, -8.22252998188221617681e-01) (15, -8.96456131893311258807e+00) (16, 4.15481842120159861054e+00) (17, 1.18536485571616334589e+00) (18, -1.78006390157687133247e+00) 
