FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.36611782254697788908e+00) (1, 3.90994566839639068334e-01) (2, 4.28760647189322885442e-01) (3, 3.88689029377404571264e-01) (4, 4.42836429339352966039e-01) (5, -5.07886349979946327693e+00) (6, -6.50200244150051953618e-01) (7, 2.32257672634942791134e-01) (8, -8.16664476425048579955e-01) (9, -1.44857728625999926386e+00) (10, 4.10859644832869574937e-01) (0, 1.03599357049710377732e-01) (1, 4.31435590085509981795e-02) (2, 3.06345855292800700598e-02) (3, 1.09753045946073865546e-01) (4, 1.10938604682398175849e-01) (5, -2.14777413241497860952e+00) (6, -4.58174794233805526833e-01) (7, 2.00335572901520680977e-01) (8, -3.37927870539489361157e-01) (9, 2.18750653869996647405e-01) (10, 1.43921557000074196164e-01) (0, 3.91166671935185117359e-01) (1, 2.55175905286000870298e-01) (2, 2.02992292104886701276e-01) (3, 2.23537849246190717389e-01) (4, 1.32816689072774524183e-01) (5, -4.29322345065658694807e+00) (6, 2.16838407409016564742e+00) (7, 5.87131613882156067064e-01) (8, 4.99796270087532956694e+00) (9, 4.93597094098022459718e+00) (10, 3.18265893864170701377e-01) (0, 2.76757232669457120533e-01) (1, 2.73325528501474235288e-01) (2, 2.16091509460412972699e-01) (3, 2.38284509180986397991e-01) (4, 1.04247826634847537419e-01) (5, -4.41903962450807696882e+00) (6, 2.21101435166422799483e+00) (7, 1.31039978358673714354e+00) (8, 3.63156307514525833469e+00) (9, 4.53172061951793470058e+00) (10, 3.74107555853859574668e-01) (0, 1.40150783451320637418e+00) (1, 4.96732492987487117553e-01) (2, 4.22881816570136348510e-01) (3, 5.35371293370101142273e-01) (4, 5.48976880554530421996e-01) (5, -5.05219033930608940608e+00) (6, -6.54078390488092464850e-01) (7, 1.97671213239946685913e-01) (8, -1.06454725284505924421e+00) (9, -1.79920358487962883665e+00) (10, 3.56337832672752430341e-01) (0, -1.62924335040373124972e-01) (1, 2.27026992826540006953e-02) (2, -5.11041826219480951887e-02) (3, -7.95969984046754016650e-04) (4, -1.04066239914409428002e-01) (5, 2.76059782042719170647e+00) (6, -2.88669992228648308874e-01) (7, -1.86565434207162150759e-01) (8, -2.75298870717946952968e-01) (9, -4.46467781815614861429e-01) (10, 3.09949891709802083373e-01) (0, 1.11653011160942039481e+00) (1, 7.35114899444844982490e-01) (2, 6.42618504333760998115e-01) (3, 7.97583487797048351631e-01) (4, 6.50976222920682690010e-01) (5, -7.46133267409953848670e+00) (6, -4.53788665398952906216e-01) (7, 1.75257520850279624458e-01) (8, -5.39634475703324345552e-01) (9, -3.03932398825146155019e+00) (10, 5.39524567334334625990e-01) (0, 1.20118252777903888884e+00) (1, 5.92104796622567763897e-01) (2, 4.53943994198613809399e-01) (3, 6.20745588396363623396e-01) (4, 4.69037037645154752941e-01) (5, -5.51131213462110292056e+00) (6, -2.29622235595955404008e-02) (7, 6.07100821225562814476e+00) (8, -5.51647846587248746886e-01) (9, 5.15587044288478857368e+00) (10, -6.41445419699727958918e-03) (0, 9.46128299317479326547e-01) (1, 5.64714816455218593383e-01) (2, 4.64890771601769725585e-01) (3, 5.80941167716357287354e-01) (4, 4.80000385139558116698e-01) (5, -5.14816073525855255610e+00) (6, -9.57272804360155715742e-01) (7, 1.33952499416692544232e-01) (8, -9.21952643243008962415e-01) (9, -1.65644456336313061762e+00) (10, 2.37258750870886575557e-01) (0, 2.92856592509611768360e-01) (1, 1.46985560034632367632e-01) (2, 2.29377928500294370195e-01) (3, 2.67116430525898673753e-01) (4, 2.51106041793942136309e-01) (5, -3.60473004255268936191e+00) (6, 1.38211114810034652045e+00) (7, 7.33350975455613784959e-01) (8, 6.42537021363306859456e+00) (9, 4.81124047318389891359e+00) (10, 3.18045956703325194415e-01) (11, -2.53012596206082240879e-01) (12, -1.05366030220596360034e-01) (13, 3.35300905083392719597e-01) (14, 3.08063907002185444206e-01) (15, -2.28847032920731524586e-01) (16, 8.35248568061405904928e-01) (17, -2.34746785554591441691e-01) (18, 1.05181824245963048692e-01) (19, -2.47341385619057579914e-01) (20, 2.56926875485421535306e-01) (21, 1.33573697173565153484e-01) 
