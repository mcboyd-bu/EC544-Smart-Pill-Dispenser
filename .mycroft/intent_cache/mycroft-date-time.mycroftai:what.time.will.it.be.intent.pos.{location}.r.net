FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=16 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (16, 6, 5.00000000000000000000e-01) (16, 6, 5.00000000000000000000e-01) (16, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.19545479626695296815e+01) (1, 4.54589742333661135376e+01) (2, 4.46006486555837688979e-01) (3, -6.64430695544102878358e+00) (4, 4.68428289675548583659e+01) (5, 4.46908043408805681906e+01) (6, 6.24979389353321934664e+00) (7, 9.20496514983905989027e+00) (8, 2.12832716449631931255e+00) (9, 2.30661478248709617134e+00) (10, 3.96453430312208587871e-01) (11, -1.07292356757536100353e+00) (12, -1.38545760027610570519e-01) (13, 4.02422802613391361604e+00) (14, -4.74941807439106788991e+00) (15, -7.66782650637962276408e-02) (0, -5.77753179264573923923e-01) (1, 9.20757341613758550913e-01) (2, -1.35319190524583327084e-01) (3, 2.85492208211486422909e-02) (4, 9.11328638918332489816e-01) (5, 8.94709525728296206637e-01) (6, -7.63215930193615332833e-01) (7, -1.72782141616409007367e-01) (8, 1.28547021707224096154e-01) (9, -3.58370665354004724890e+00) (10, 5.27623471414677247537e-01) (11, 4.71543632809992718791e+00) (12, 1.03507077853023488068e+00) (13, 4.41790780244124547949e+00) (14, 5.14108051942885602159e+00) (15, 3.71144803520422894572e-01) (0, 4.08413711199093398818e+00) (1, 3.35285390179631437313e-01) (2, 1.06543640981281373392e+00) (3, 9.09398872325194340682e-01) (4, -1.01602642826975758661e-02) (5, -5.28182600242984134908e-01) (6, -2.34015178466591748929e+01) (7, -6.32800474949492208765e+00) (8, -4.12313818121994568600e+00) (9, 3.23201539022841943805e+00) (10, -4.78693439890002458981e-01) (11, -9.63029277169698616490e+00) (12, -2.40713654918961861284e+00) (13, -9.78298260425559185194e+00) (14, -1.48725222221358279739e+01) (15, 1.88525359863875130451e+00) (16, 9.05274130184805514432e+00) (17, -4.34515575301586096657e+01) (18, 8.79741444184640641879e+00) (19, -1.02963328937292786236e+00) 
