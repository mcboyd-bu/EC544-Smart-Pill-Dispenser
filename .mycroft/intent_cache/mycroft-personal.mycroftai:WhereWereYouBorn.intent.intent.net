FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.60458064642277142831e-01) (1, 2.29629945715405209272e-01) (2, 2.97965605517365172972e-01) (3, 1.89005695988871319502e-01) (4, 1.54753719230629666059e-01) (5, -2.66488486738887075944e+00) (6, 1.26613050591210529738e+00) (7, 1.37999551464231873243e-01) (8, 1.56265357225571555055e+00) (9, 4.69351388813981440506e+00) (10, 5.06317541856192709915e-01) (0, 8.49416675883756577825e-01) (1, 2.58721736142487390708e-01) (2, 2.42805142769665499580e-01) (3, 3.26198709080071425070e-01) (4, 2.11440396377892414481e-01) (5, 4.47017798025601997836e+00) (6, -2.09305054735274609357e+00) (7, -4.65702888153868288512e-02) (8, -2.16237604411080708289e+00) (9, -2.39698509772927437211e+00) (10, 4.62211140644323836835e-01) (0, 9.70800291444595342405e-01) (1, 2.51773582832284514854e-01) (2, 3.28317703740068078666e-01) (3, 2.20693567828603387504e-01) (4, 2.02686807201572061210e-01) (5, -2.59802733572211508317e+00) (6, 1.26654282104233906203e+00) (7, 1.53203057218702698439e-01) (8, 7.88908713190104471380e-01) (9, 4.69463816585072901688e+00) (10, 3.77760050847790629369e-01) (0, 1.37979075078965451873e+00) (1, 5.89272832742949725926e-01) (2, 6.26655569545050861180e-01) (3, 7.11754082015772837622e-01) (4, 5.82945024839659931004e-01) (5, 9.58697468762724214209e-02) (6, 1.81542002000860236421e+00) (7, 9.26082453363935798052e-01) (8, 3.47851607958090935924e+00) (9, -3.93195229278312741172e+00) (10, 1.34988746897432609906e+00) (0, 5.87041142388279868491e-01) (1, 4.66598154364382289305e-01) (2, 4.61133965609823781584e-01) (3, 4.62365233658110219572e-01) (4, 5.03196643052851166900e-01) (5, 7.50305149701186779865e-01) (6, 3.31767894846708832901e+00) (7, 8.94273133923364604847e-01) (8, 1.56809755140646656102e+00) (9, -3.26392445335535752449e+00) (10, 1.98814386319746261300e+00) (0, -1.18214026872579069938e+00) (1, -1.83145180285143427401e-01) (2, -1.53103664457964472323e-01) (3, -2.16646254003214411288e-01) (4, -1.89060643255877069979e-01) (5, 1.82549399421426516987e+00) (6, 7.11891633136860990660e-01) (7, -2.29915882352943079514e-01) (8, 7.18465840286776846746e-01) (9, 5.64333786970786066561e-01) (10, -4.12037024004306029479e-01) (0, 1.05180219903164684503e+00) (1, 3.27342595157356586189e-01) (2, 1.59329506752224320243e-01) (3, 3.08893705901832904548e-01) (4, 2.08242199567289704154e-01) (5, -2.67094806792226080461e+00) (6, 8.85558719071007471868e-01) (7, 5.28134425174316482487e-02) (8, 1.08790278127572381806e+00) (9, 4.64167431296834376298e+00) (10, 4.15884679212830454809e-01) (0, -1.25378597249768786170e+00) (1, -1.79405882873573258030e-01) (2, -1.75851270475425675022e-01) (3, -2.70770631649532522722e-01) (4, -3.23627181270160879656e-01) (5, 2.44685023087339814651e+00) (6, 7.10725821995663253894e-01) (7, -4.10033467252561290017e-01) (8, 7.64602172903638188650e-01) (9, 6.79961686736754389315e-01) (10, -6.00238440589410005011e-01) (0, -1.30806076993726261293e+00) (1, -2.67978951253929342791e-01) (2, -3.15695598640480246111e-01) (3, -2.29267455556430826968e-01) (4, -1.66231557645835831272e-01) (5, 2.42085993844347413528e+00) (6, 8.01560285097471014737e-01) (7, -1.93808150686422153708e-01) (8, 7.43018388791495731915e-01) (9, 6.74153752992857846493e-01) (10, -4.96153580055934295689e-01) (0, 4.56632849014159791778e-01) (1, -2.84975091026343843592e-02) (2, -2.90785873339690706385e-02) (3, 1.35404557545372061789e-01) (4, -3.01848346398391302603e-02) (5, -1.06530055983520366425e+00) (6, -1.15164606601279889686e-01) (7, 5.49496898047226567385e-01) (8, -3.01332543902174010420e-01) (9, 5.41310735795173414431e-01) (10, 1.19253355775192831922e-01) (11, 5.00554457762140470045e-01) (12, 5.43884216030596223668e-01) (13, 4.55636084535021079756e-01) (14, -1.63774573435548048694e-01) (15, -2.23059253027203835407e-01) (16, 7.26066786789170803829e-01) (17, 4.28125344672102225996e-01) (18, 5.96456799027811901404e-01) (19, 6.01654169346358735915e-01) (20, -1.35138149584559025262e-01) (21, 4.20193564641712702379e-01) 
